# Training Guide - TypeScript Rewards in Python

## Quick Start

Your Python training now uses the **exact same reward system** as your TypeScript browser training!

### Start Training

```bash
cd python_training

# Activate virtual environment
venv\Scripts\activate

# Start training with TypeScript rewards
python train_sb3.py
```

That's it! The training will now use all your sophisticated TypeScript rewards.

---

## What's Different?

### Before (Old Python System):
- ‚ùå Simple placeholder rewards
- ‚ùå No progression tracking
- ‚ùå No smooth control rewards
- ‚ùå 28-dimension observation space
- ‚ùå Single orb only

### Now (TypeScript Rewards Ported):
- ‚úÖ Full TypeScript reward system
- ‚úÖ Progression tracking (grab‚Üítransport‚Üídeposit‚Üíreturn)
- ‚úÖ Smooth control rewards
- ‚úÖ 30-dimension observation space (normalized)
- ‚úÖ Multi-orb support (up to 15 orbs)
- ‚úÖ All 4 training stages available

---

## Monitor Training

### Option 1: Watch the Terminal
Training progress is printed every 10 episodes with:
- Episode reward
- Episode length
- Orbs collected & deposited
- Training FPS

### Option 2: Angular Dashboard (if you set it up)
Open `http://localhost:3000` to see real-time charts.

---

## Checkpoints

Models are saved every **10,000 episodes** in:
```
python_training/checkpoints/model_episode_10000.zip
python_training/checkpoints/model_episode_20000.zip
python_training/checkpoints/model_episode_30000.zip
...
python_training/checkpoints/final_model.zip
```

---

## Test Your Models Visually

Once you have a trained model:

### 1. Start the inference server
```bash
cd python_training
venv\Scripts\python inference_server.py --model checkpoints/final_model
```

### 2. Open the Angular app
```bash
cd webapp/ml-navigation
ng serve
```
Open `http://localhost:4200`

### 3. Load and watch!
- Select "AI Control" mode
- Browse to your model .zip file
- Click "Load Model & Start AI"
- **Watch your trained rover navigate!**

---

## Training Tips

### Faster Training
The current config trains for a while. To speed up testing:
1. Edit `python_training/config/default_config.yaml`
2. Change `max_episodes: 10000` to something smaller like `1000`
3. Restart training

### Change Checkpoint Frequency
In `train_sb3.py` line 41:
```python
self.checkpoint_interval = 10000  # Change this number
```

### Monitor GPU Usage
If you have a GPU, PyTorch should automatically use it for faster training.

---

## Reward System Features

Your models now learn:

### Progression Rewards:
- ‚úÖ Grab orbs (+15-20 points)
- ‚úÖ Leave excavation with orbs (+40-50)
- ‚úÖ Enter construction with orbs (+100-150)
- ‚úÖ Deposit in berm (+2000) or construction (+1000)
- ‚úÖ Return to excavation (+150)

### Smooth Control:
- ‚úÖ Smooth acceleration and turning
- ‚úÖ Maintaining consistent speed
- ‚úÖ Maintaining consistent heading
- ‚úÖ High-speed efficiency

### Penalties:
- ‚úÖ Collision penalty (-1000)
- ‚úÖ Time pressure (-0.2 per step)
- ‚úÖ Idle penalty (-3.0)
- ‚úÖ Backward movement penalty (-8.0)
- ‚úÖ Wasteful orb drops (-300)

### Intelligence:
- ‚úÖ Orb swap rewards (drop 3, grab 5 = smart)
- ‚úÖ Oscillation detection (penalize jerky movement)
- ‚úÖ Zone-aware holding rewards

---

## Troubleshooting

**"ModuleNotFoundError"**
- Make sure you activated the venv: `venv\Scripts\activate`

**"Observation space mismatch"**
- The new environment is 30-dim, old models were 28-dim
- You need to train new models with the updated system

**"NaN rewards"**
- Check that the environment is creating properly
- Run: `python test_env.py` to verify

**Training is slow**
- Check CPU usage (should be near 100%)
- Consider reducing `max_steps_per_episode` in config

---

## Next Steps

1. **Train a model** with the new TypeScript rewards
2. **Test it** using the terminal: `python play_model.py --model checkpoints/final_model`
3. **Visualize it** in the browser with the inference server
4. **Compare** performance to your browser-trained models

Your Python training is now **identical** to your TypeScript browser training, but **10-100x faster**! üöÄ
